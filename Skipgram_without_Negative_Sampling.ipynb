{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qRlR7iXT7lqb"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import spacy as sp\n",
    "from tqdm import tqdm\n",
    "# useful stuff\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uug5FfEn7lqf"
   },
   "outputs": [],
   "source": [
    "nlp = sp.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-ryprx97lqj"
   },
   "outputs": [],
   "source": [
    "def text2sentences(path):\n",
    "    sentences = []\n",
    "    string=''\n",
    "    with open(path,encoding=\"utf8\") as f:\n",
    "        content=f.read()\n",
    "        docs_raw = content.splitlines()\n",
    "        for l in tqdm(docs_raw):\n",
    "            x=nlp(l.lower())\n",
    "            string_tokens = [token.orth_ for token in x if not token.is_punct]\n",
    "            sentences.append(string_tokens)\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9NUhZEwH7lqm",
    "outputId": "adbf81ea-8a22-443c-ecf8-a837f3d6047e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15128/15128 [02:24<00:00, 104.92it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path='/content/news.en-00001-of-00100'\n",
    "sentences = text2sentences(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ldzQpUk7lqq"
   },
   "outputs": [],
   "source": [
    "sentences = sentences[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRs_mDLE7lq3"
   },
   "outputs": [],
   "source": [
    "class SkipGram:\n",
    "    def __init__(self, sentences, nEmbed=100, negativeRate=5, winSize = 5, minCount = 5):\n",
    "        self.vocab={}\n",
    "        for line in sentences:\n",
    "          for word in line:\n",
    "            if word not in self.vocab:\n",
    "                self.vocab[word]=1\n",
    "        self.w2id = dict((i,word) for word,i in enumerate(self.vocab))\n",
    "        self.trainset=dict((i,line) for i,line in enumerate(sentences))\n",
    "        self.weight_1=np.random.uniform(-1,1,(len(self.vocab.keys()),nEmbed))\n",
    "        self.weight_2=np.random.uniform(-1,1,(nEmbed,len(self.vocab.keys())))\n",
    "        self.error = 0\n",
    "        self.train() \n",
    "        \n",
    "    def onehotcode(self,word):\n",
    "      word_onehot= []\n",
    "      count=len(self.vocab.keys())\n",
    "      vec=np.zeros(count)\n",
    "      pos=self.w2id[word]\n",
    "      vec[pos]=1\n",
    "      return vec\n",
    "\n",
    "    def train(self):\n",
    "        for counter,sentence in tqdm(self.trainset.items()):\n",
    "            sentence = list(filter(lambda word: word in self.vocab, sentence))#check if all words of sentence are in vocab\n",
    "            for wpos,word in tqdm(enumerate(sentence)):\n",
    "                \n",
    "                wIdx = self.w2id[word]\n",
    "                winsize = 2 \n",
    "                start = max(0, wpos - winsize)\n",
    "                end = min(wpos + winsize + 1, len(sentence))\n",
    "                word_vec=self.onehotcode(word)\n",
    "                context_vec=[]\n",
    "                train_vec=[]\n",
    "                \n",
    "                for context_word in sentence[start:end]: \n",
    "                    ctxtId = self.w2id[context_word]\n",
    "                    if ctxtId == wIdx: continue\n",
    "                    context_vec.append(self.onehotcode(context_word))\n",
    "                train_vec.append([word_vec,context_vec])               \n",
    "                self.trainWord(train_vec)#call here\n",
    "    \n",
    "    def trainWord(self, train_vec):\n",
    "        for i in (range(5)):\n",
    "            self.error = 0\n",
    "            for word,context in train_vec:\n",
    "                pred,h,o=self.forward(word)\n",
    "                for contextvec in context:\n",
    "                    subarray=pred-contextvec\n",
    "                    self.error+=subarray\n",
    "                self.backprop(h,word,self.error)\n",
    "        \n",
    "                    \n",
    "    def backprop(self,h,word,error):\n",
    "        up1=np.outer(h,error)\n",
    "        up2=np.outer(word,np.dot(self.weight_2,error.T))\n",
    "        self.weight_1=self.weight_1-(0.2*up2)\n",
    "        self.weight_2=self.weight_2-(0.2*up1)\n",
    "                \n",
    "    def similarity(self,word1,word2):\n",
    "        vec1=self.weight_1[self.w2id[word1]]\n",
    "        vec2=self.weight_1[self.w2id[word2]]\n",
    "        vec_sum=np.dot(vec1,vec2)\n",
    "        vec_norm=np.linalg.norm(vec1)*np.linalg.norm(vec2)\n",
    "        cosine_dist=vec_sum/vec_norm\n",
    "        return(cosine_dist)                                     \n",
    "                              \n",
    "    def forward(self,w):\n",
    "        hidden=np.dot(self.weight_1.T,w)\n",
    "        output=np.dot(self.weight_2.T,hidden)\n",
    "        pred=self.softmax(output)\n",
    "        return(pred,hidden,output)\n",
    "        \n",
    "        \n",
    "    def softmax(self,x):\n",
    "        z=np.exp(x-np.max(x))\n",
    "        return(z/z.sum())                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "fj4wAQIdGg9f",
    "outputId": "e934219b-6133-4a47-dff5-0bd72770f0de",
    "scrolled": false
   },
   
